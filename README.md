# char-boltzmann

Character-level RBMs for short text. For more information, check out [my blog post](https://colinmorris.github.io/blog/dreaming-rbms).

# How-to

The two important scripts are:

- `train.py`: trains an RBM model on a text file with one short text per line. It has a whole bunch of command line options you can supply, but the defaults are all pretty reasonable. The one you're most likely to need to change is `--extra-chars` - the default behaviour is to use only [a-z ] (and [A-Z] implicitly downcased), which is definitely not appropriate for some datasets having lots of numerals/punctuation.
- `sample.py`: generates new short texts given a pickled model file generated by `train.py`

(The last script, `compare_models.py` is only really relevant if you're training a bunch of different models on the same dataset and enjoy spreadsheets.)

More details on the arguments to these scripts can be seen by running them with '-h'.

README-datasets.md has pointers to some suitable datasets. If you want something small to play with, the first names dataset is tiny (in the vertical and horizontal direction), so you can train a pretty reasonable model on it in, say, 30 minutes (setting --maxlen to something small like 10).

# Requirements

scikit-learn and its dependencies (numpy, scipy) is the big one. Also enum34. `pip install -r requirements.txt` might be all you need to do.

# More details

The core RBM code is cannibalized from scikit-learn's [BernoulliRBM](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM) implementation. I tacked on some additional features including:

- L2 weight cost
- softmax sampling
- sampling with temperature (for simulated annealing)
- flag to gradually reduce learning rate
- initializing visible biases to the training set means

This code has the same performance limitations as the base sklearn implementation. In particular, it can't run on a GPU.

The 'workspace' branch has a lot of extra scripts and data files which *might* be useful to someone, but which are kind of messy (even relative to the already-kinda-messy master). They mostly relate to model visualization and experiments with different sampling techniques.
